{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import gensim\n",
    "import numpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.preprocessing as sequence\n",
    "from keras import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DATA PREPARATION CODE\n",
    "\"\"\"\n",
    "df = pandas.read_json(\"yelp_academic_dataset_review.json\", lines=True)\n",
    "df.drop(['review_id', 'user_id', 'business_id', 'useful', 'funny',\n",
    "       'cool', 'date'], axis=1, inplace=True)     # drop useless data from frame to save space, time\n",
    "df = df[df.stars.isnull() == False]\n",
    "df['stars'] = df['stars'].map(int)\n",
    "df = df[df.text.isnull() == False]\n",
    "print('dataset loaded with shape:', df.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tokenize_review(review):\n",
    "    \"\"\"Function to tokenize each review\"\"\"\n",
    "    review = review.lower()  # convert to lowercase\n",
    "    tokens = word_tokenize(review)  # use punkt to tokenize review\n",
    "    # tokens = [x for x in tokens if x not in string.punctuation] # step to remove punctuation\n",
    "    # tokens = [x for x in tokens if x not in stop_words] # step to remove stopwords\n",
    "    return tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def postprocess(data, n=1000000):\n",
    "    \"\"\"Function to process reviews for Gensim W2V.\"\"\"\n",
    "    data = data.head(n)\n",
    "    data['tokens'] = data['text'].progress_map(tokenize_review)\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = postprocess(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initializing pre-trained Word2Vec embedding model\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Generating embeddings for all the reviews in the dataset\n",
    "\n",
    "data = list()\n",
    "# For each review in the corpus\n",
    "for row in range(len(df)):\n",
    "    review = []\n",
    "    # For each word in the review\n",
    "    for w in df['review'][row].split():\n",
    "        # Append the w2v vector for the word to the review embedding\n",
    "        try:\n",
    "            review.append(w2v_model.get_vector(w.lower()))\n",
    "        except KeyError:\n",
    "            continue\n",
    "    data.append(review)\n",
    "data = numpy.array(data)\n",
    "labels = numpy.array(df['label'])\n",
    "\n",
    "print(len(data))\n",
    "print(len(labels))\n",
    "print(len(data[0][0]))\n",
    "\n",
    "vocab_size=3000000\n",
    "embedding_dim=300\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Neural network classifier model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_dim=embedding_dim))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Shuffle the data and labels *in the same order* to prevent overfitting\n",
    "seed = numpy.random.get_state()\n",
    "numpy.random.shuffle(data)\n",
    "numpy.random.set_state(seed)\n",
    "numpy.random.shuffle(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_input_words = 0\n",
    "# Find the longest review to pad the others to match its length\n",
    "for row in range(len(df)):\n",
    "    if len(df['review'][row].split()) > max_input_words:\n",
    "        max_input_words = len(df['review'][row].split())\n",
    "\n",
    "print(max_input_words)\n",
    "\n",
    "if max_input_words == 0:\n",
    "    max_input_words = 500"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "traind, vald, trainl, vall = train_test_split(data, labels)\n",
    "traind = sequence.pad_sequences(traind, maxlen=max_input_words)\n",
    "vald = sequence.pad_sequences(vald, maxlen=max_input_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.fit(traind, trainl, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.evaluate(vald, vall, verbose=1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}